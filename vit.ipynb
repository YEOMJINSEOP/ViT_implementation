{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Image를 patch로 Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([8, 3, 224, 224])\n",
      "patches :  torch.Size([8, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(8, 3, 224, 224) # batch_size=8, channel=3, h=224, w=224\n",
    "print(f'x: {x.shape}')\n",
    "\n",
    "patch_size = 16 # 16 x 16 pixel patch\n",
    "patches = rearrange(x, 'b c (h s1) (w s2) -> b (h w) (s1 s2 c)',\n",
    "                    s1=patch_size, s2=patch_size)\n",
    "print(f'patches : ', patches.shape)                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 768, 14, 14]         590,592\n",
      "         Rearrange-2             [-1, 196, 768]               0\n",
      "================================================================\n",
      "Total params: 590,592\n",
      "Trainable params: 590,592\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 2.30\n",
      "Params size (MB): 2.25\n",
      "Estimated Total Size (MB): 5.12\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 위와 같이 rearrange를 통해 단순히 reshape을 해줄 수 있지만, \n",
    "# 아래와 같이 Conv를 이용해 patch를 만들면, 성능에 이점이 있다고 합니다.\n",
    "\n",
    "patch_size = 16\n",
    "in_channels = 3\n",
    "emb_size = 768 # channel * patch_size * patch_size\n",
    "\n",
    "# using a conv layer instaed of a linear one => performance gains\n",
    "projection = nn.Sequential(\n",
    "  nn.Conv2d(in_channels, emb_size,\n",
    "            kernel_size=patch_size, stride=patch_size), # torch.Size([8, 768, 14, 14])\n",
    "  Rearrange('b e (h) (w) -> b (h w) e') # torch.Size([8, 196,768])\n",
    ")\n",
    "\n",
    "summary(projection, x.shape[1:], device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Class token 추가 및 Positional Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projected X shape: torch.Size([8, 196, 768])\n",
      "cls : torch.Size([1, 1, 768]), pos: torch.Size([197, 768])\n",
      "Repeated Cls shape: torch.Size([8, 1, 768])\n",
      "output: torch.Size([8, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "emb_size = 768\n",
    "img_size = 224\n",
    "patch_size = 16\n",
    "\n",
    "# image를 patch_size로 나누고 flatten\n",
    "projected_x = projection(x)\n",
    "print(f'Projected X shape: {projected_x.shape}') # torch.Size([8, 196, 768])\n",
    "\n",
    "# cls_token과 pos encoding Parameter 정의\n",
    "cls_token = nn.Parameter(torch.randn(1,1, emb_size)) # torch.Size([1, 1, 768])\n",
    "positions = nn.Parameter(torch.randn((img_size // patch_size) ** 2 + 1,emb_size)) # torch.Size([197, 768])\n",
    "print(f'cls : {cls_token.shape}, pos: {positions.shape}')\n",
    "\n",
    "# cls_token을 반복하여 batch_size의 크기와 맞춰줌.\n",
    "batch_size = 8\n",
    "cls_tokens = repeat(cls_token, '() n e -> b n e', b=batch_size) # torch.Size(8, 1, 768)\n",
    "print(f'Repeated Cls shape: {cls_tokens.shape}') \n",
    "\n",
    "# cls_token과 projected_x를 concatenate\n",
    "cat_x = torch.cat([cls_tokens, projected_x], dim=1) # torch.Size([8, 197, 768])\n",
    "\n",
    "# position encoding을 더해줌\n",
    "cat_x += positions \n",
    "print(f'output: {cat_x.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 + 2 ) Patch Projection, Positional Encoding을 Class 형태로 만들어줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "  def __init__(self, in_channels: int=3, patch_size: int=16,\n",
    "                emb_size: int=768, img_size: int=224):\n",
    "      self.patch_size = patch_size\n",
    "      super().__init__()\n",
    "      self.projection = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size),\n",
    "        Rearrange('b e (h) (w) -> b (h w) e')\n",
    "      )\n",
    "      self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\n",
    "      self.positions = nn.Parameter(torch.randn((img_size // patch_size) ** 2 + 1, emb_size))\n",
    "\n",
    "  def forward(self, x: Tensor) -> Tensor:\n",
    "    batch_size = x.shape[0] # batch_size\n",
    "    x = self.projection(x)\n",
    "    cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=batch_size)\n",
    "    # prepend the cls_token to the input\n",
    "    x = torch.cat([cls_tokens, x], dim=1)\n",
    "    # add position embedding\n",
    "    x += self.positions # torch.size([8, 197, 768])\n",
    "\n",
    "    return x\n",
    "      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 768, 14, 14]         590,592\n",
      "         Rearrange-2             [-1, 196, 768]               0\n",
      "================================================================\n",
      "Total params: 590,592\n",
      "Trainable params: 590,592\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 2.30\n",
      "Params size (MB): 2.25\n",
      "Estimated Total Size (MB): 5.12\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "PE = PatchEmbedding()\n",
    "summary(PE, (3, 224, 224), device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi Head Attention (MHA)\n",
    "\n",
    "patch들에 대해 Self-Attention 메커니즘을 적용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys: Linear(in_features=768, out_features=768, bias=True),\n",
      "queries: Linear(in_features=768, out_features=768, bias=True),\n",
      "values: Linear(in_features=768, out_features=768, bias=True)\n",
      "\n",
      "queries(x): torch.Size([8, 197, 768])\n",
      "\n",
      "queires: torch.Size([8, 8, 197, 96])\n",
      "keys: torch.Size([8, 8, 197, 96])\n",
      "values: torch.Size([8, 8, 197, 96])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(8, 3, 224, 224) # batch_size=8, channel=3, h=224, w=224\n",
    "emb_size = 768\n",
    "num_heads = 8\n",
    "\n",
    "keys = nn.Linear(emb_size, emb_size)\n",
    "queries = nn.Linear(emb_size, emb_size)\n",
    "values = nn.Linear(emb_size, emb_size)\n",
    "print(f'keys: {keys},\\nqueries: {queries},\\nvalues: {values}\\n')\n",
    "\n",
    "x = PE(x)\n",
    "print(f'queries(x): {queries(x).shape}\\n') # torch.Size([8, 197, 768]) = [batch, n, emb_size]\n",
    "queries = rearrange(queries(x), 'b n (h d) -> b h n d', h=num_heads)\n",
    "keys = rearrange(keys(x), 'b n (h d) -> b h n d', h=num_heads)\n",
    "values = rearrange(values(x), 'b n (h d) -> b h n d', h=num_heads)\n",
    "\n",
    "print(f'queires: {queries.shape}\\nkeys: {keys.shape}\\nvalues: {values.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "energy: torch.Size([8, 8, 197, 197])\n",
      "att: torch.Size([8, 8, 197, 197])\n",
      "out: torch.Size([8, 8, 197, 96])\n",
      "rearranged_out:  torch.Size([8, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "# Queries * Keys\n",
    "energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)\n",
    "print(f'energy: {energy.shape}')\n",
    "\n",
    "# Get Attention Score\n",
    "scaling = emb_size ** (1/2)\n",
    "att = F.softmax(energy/scaling, dim=-1)\n",
    "print(f'att: {att.shape}')\n",
    "\n",
    "# Attention Score * Values\n",
    "out = torch.einsum('bhal, bhlv -> bhav', att, values)\n",
    "print('out:', out.shape)\n",
    "\n",
    "# Rearrange to emb_size\n",
    "out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "print('rearranged_out: ', out.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiHead Attention을 Class로 묶어주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, emb_size: int = 768, num_heads: int = 8, dropout: float = 0):\n",
    "    super().__init__()\n",
    "    self.emb_size = emb_size\n",
    "    self.num_heads = num_heads\n",
    "    # fuse the queries, keys and values in one matrix\n",
    "    self.qkv = nn.Linear(emb_size, emb_size*3)\n",
    "    self.att_drop = nn.Dropout(dropout)\n",
    "    self.projection = nn.Linear(emb_size, emb_size)\n",
    "  \n",
    "  def forward(self, x: Tensor, mask: Tensor = None) -> Tensor:\n",
    "    # split queries, keys and vlaues in num_head\n",
    "    qkv = rearrange(self.qkv(x), 'b n (h d qkv) -> (qkv) b h n d', h = self.num_heads, qkv=3)\n",
    "    queries, keys, values = qkv[0], qkv[1], qkv[2]\n",
    "    # sum up over the last axis\n",
    "    energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys) # batch, num_heads, query_len, key_len\n",
    "    # energy: [8, 8, 197, 197]\n",
    "\n",
    "    if mask is not None:\n",
    "      fill_value = torch.finfo(torch.float32).min\n",
    "      energy.mask_fill(~mask, fill_value)\n",
    "\n",
    "    scaling = self.emb_size ** (1/2)\n",
    "    att = F.softmax(energy/scaling, dim=-1)\n",
    "    att = self.att_drop(att)\n",
    "\n",
    "    # sum up over the third axis\n",
    "    out = torch.einsum('bhal, bhlv -> bhav', att, values)\n",
    "    out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "    out = self.projection(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 197, 768])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1            [-1, 197, 2304]       1,771,776\n",
      "           Dropout-2          [-1, 8, 197, 197]               0\n",
      "            Linear-3             [-1, 197, 768]         590,592\n",
      "================================================================\n",
      "Total params: 2,362,368\n",
      "Trainable params: 2,362,368\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.58\n",
      "Forward/backward pass size (MB): 6.99\n",
      "Params size (MB): 9.01\n",
      "Estimated Total Size (MB): 16.57\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(8, 3, 224, 224)\n",
    "PE = PatchEmbedding()\n",
    "x = PE(x)\n",
    "print(x.shape)\n",
    "MHA = MultiHeadAttention()\n",
    "summary(MHA, x.shape[1:], device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transformer Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualAdd(nn.Module):\n",
    "  def __init__(self, fn):\n",
    "    super().__init__()\n",
    "    self.fn = fn\n",
    "\n",
    "  def forward(self, x, **kwargs):\n",
    "    res = x\n",
    "    x = self.fn(x, **kwargs)\n",
    "    x += res\n",
    "    return x\n",
    "\n",
    "class FeedForwardBlock(nn.Sequential):\n",
    "  def __init__(self, emb_size: int, expansion: int = 4, drop_p: float = 0.):\n",
    "    super().__init__(\n",
    "      nn.Linear(emb_size, expansion * emb_size),\n",
    "      nn.GELU(),\n",
    "      nn.Dropout(drop_p),\n",
    "      nn.Linear(expansion * emb_size, emb_size),\n",
    "    )\n",
    "\n",
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "  def __init__(self, emb_size: int = 768, drop_p: float = 0., \n",
    "               forward_expansion: int = 4, forward_drop_p: float=0.,\n",
    "               **kwargs):\n",
    "      super().__init__(\n",
    "        ResidualAdd(\n",
    "          nn.Sequential(\n",
    "            nn.LayerNorm(emb_size),\n",
    "            MultiHeadAttention(emb_size, **kwargs),\n",
    "            nn.Dropout(drop_p)\n",
    "        )),\n",
    "        ResidualAdd(\n",
    "          nn.Sequential(\n",
    "            nn.LayerNorm(emb_size),\n",
    "            FeedForwardBlock(\n",
    "              emb_size, expansion=forward_expansion, drop_p=forward_drop_p\n",
    "            ),\n",
    "            nn.Dropout(drop_p),\n",
    "          )\n",
    "        )\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         LayerNorm-1             [-1, 197, 768]           1,536\n",
      "            Linear-2            [-1, 197, 2304]       1,771,776\n",
      "           Dropout-3          [-1, 8, 197, 197]               0\n",
      "            Linear-4             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-5             [-1, 197, 768]               0\n",
      "           Dropout-6             [-1, 197, 768]               0\n",
      "       ResidualAdd-7             [-1, 197, 768]               0\n",
      "         LayerNorm-8             [-1, 197, 768]           1,536\n",
      "            Linear-9            [-1, 197, 3072]       2,362,368\n",
      "             GELU-10            [-1, 197, 3072]               0\n",
      "          Dropout-11            [-1, 197, 3072]               0\n",
      "           Linear-12             [-1, 197, 768]       2,360,064\n",
      "          Dropout-13             [-1, 197, 768]               0\n",
      "      ResidualAdd-14             [-1, 197, 768]               0\n",
      "================================================================\n",
      "Total params: 7,087,872\n",
      "Trainable params: 7,087,872\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.58\n",
      "Forward/backward pass size (MB): 30.07\n",
      "Params size (MB): 27.04\n",
      "Estimated Total Size (MB): 57.69\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "x  = torch.randn(8, 3, 224, 224)\n",
    "x = PE(x)\n",
    "x = MHA(x)\n",
    "TE = TransformerEncoderBlock()\n",
    "summary(TE, x.shape[1:], device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ViT에는 이런 Encoder block이 12개가 있다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 마지막으로 다 묶어서 ViT 빌드\n",
    "classification을 위한 ClassificationHead를 만들어 모델의 마지막 단에 넣어준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 768, 14, 14]         590,592\n",
      "         Rearrange-2             [-1, 196, 768]               0\n",
      "    PatchEmbedding-3             [-1, 197, 768]               0\n",
      "         LayerNorm-4             [-1, 197, 768]           1,536\n",
      "            Linear-5            [-1, 197, 2304]       1,771,776\n",
      "           Dropout-6          [-1, 8, 197, 197]               0\n",
      "            Linear-7             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-8             [-1, 197, 768]               0\n",
      "           Dropout-9             [-1, 197, 768]               0\n",
      "      ResidualAdd-10             [-1, 197, 768]               0\n",
      "        LayerNorm-11             [-1, 197, 768]           1,536\n",
      "           Linear-12            [-1, 197, 3072]       2,362,368\n",
      "             GELU-13            [-1, 197, 3072]               0\n",
      "          Dropout-14            [-1, 197, 3072]               0\n",
      "           Linear-15             [-1, 197, 768]       2,360,064\n",
      "          Dropout-16             [-1, 197, 768]               0\n",
      "      ResidualAdd-17             [-1, 197, 768]               0\n",
      "        LayerNorm-18             [-1, 197, 768]           1,536\n",
      "           Linear-19            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-20          [-1, 8, 197, 197]               0\n",
      "           Linear-21             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-22             [-1, 197, 768]               0\n",
      "          Dropout-23             [-1, 197, 768]               0\n",
      "      ResidualAdd-24             [-1, 197, 768]               0\n",
      "        LayerNorm-25             [-1, 197, 768]           1,536\n",
      "           Linear-26            [-1, 197, 3072]       2,362,368\n",
      "             GELU-27            [-1, 197, 3072]               0\n",
      "          Dropout-28            [-1, 197, 3072]               0\n",
      "           Linear-29             [-1, 197, 768]       2,360,064\n",
      "          Dropout-30             [-1, 197, 768]               0\n",
      "      ResidualAdd-31             [-1, 197, 768]               0\n",
      "        LayerNorm-32             [-1, 197, 768]           1,536\n",
      "           Linear-33            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-34          [-1, 8, 197, 197]               0\n",
      "           Linear-35             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-36             [-1, 197, 768]               0\n",
      "          Dropout-37             [-1, 197, 768]               0\n",
      "      ResidualAdd-38             [-1, 197, 768]               0\n",
      "        LayerNorm-39             [-1, 197, 768]           1,536\n",
      "           Linear-40            [-1, 197, 3072]       2,362,368\n",
      "             GELU-41            [-1, 197, 3072]               0\n",
      "          Dropout-42            [-1, 197, 3072]               0\n",
      "           Linear-43             [-1, 197, 768]       2,360,064\n",
      "          Dropout-44             [-1, 197, 768]               0\n",
      "      ResidualAdd-45             [-1, 197, 768]               0\n",
      "        LayerNorm-46             [-1, 197, 768]           1,536\n",
      "           Linear-47            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-48          [-1, 8, 197, 197]               0\n",
      "           Linear-49             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-50             [-1, 197, 768]               0\n",
      "          Dropout-51             [-1, 197, 768]               0\n",
      "      ResidualAdd-52             [-1, 197, 768]               0\n",
      "        LayerNorm-53             [-1, 197, 768]           1,536\n",
      "           Linear-54            [-1, 197, 3072]       2,362,368\n",
      "             GELU-55            [-1, 197, 3072]               0\n",
      "          Dropout-56            [-1, 197, 3072]               0\n",
      "           Linear-57             [-1, 197, 768]       2,360,064\n",
      "          Dropout-58             [-1, 197, 768]               0\n",
      "      ResidualAdd-59             [-1, 197, 768]               0\n",
      "        LayerNorm-60             [-1, 197, 768]           1,536\n",
      "           Linear-61            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-62          [-1, 8, 197, 197]               0\n",
      "           Linear-63             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-64             [-1, 197, 768]               0\n",
      "          Dropout-65             [-1, 197, 768]               0\n",
      "      ResidualAdd-66             [-1, 197, 768]               0\n",
      "        LayerNorm-67             [-1, 197, 768]           1,536\n",
      "           Linear-68            [-1, 197, 3072]       2,362,368\n",
      "             GELU-69            [-1, 197, 3072]               0\n",
      "          Dropout-70            [-1, 197, 3072]               0\n",
      "           Linear-71             [-1, 197, 768]       2,360,064\n",
      "          Dropout-72             [-1, 197, 768]               0\n",
      "      ResidualAdd-73             [-1, 197, 768]               0\n",
      "        LayerNorm-74             [-1, 197, 768]           1,536\n",
      "           Linear-75            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-76          [-1, 8, 197, 197]               0\n",
      "           Linear-77             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-78             [-1, 197, 768]               0\n",
      "          Dropout-79             [-1, 197, 768]               0\n",
      "      ResidualAdd-80             [-1, 197, 768]               0\n",
      "        LayerNorm-81             [-1, 197, 768]           1,536\n",
      "           Linear-82            [-1, 197, 3072]       2,362,368\n",
      "             GELU-83            [-1, 197, 3072]               0\n",
      "          Dropout-84            [-1, 197, 3072]               0\n",
      "           Linear-85             [-1, 197, 768]       2,360,064\n",
      "          Dropout-86             [-1, 197, 768]               0\n",
      "      ResidualAdd-87             [-1, 197, 768]               0\n",
      "        LayerNorm-88             [-1, 197, 768]           1,536\n",
      "           Linear-89            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-90          [-1, 8, 197, 197]               0\n",
      "           Linear-91             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-92             [-1, 197, 768]               0\n",
      "          Dropout-93             [-1, 197, 768]               0\n",
      "      ResidualAdd-94             [-1, 197, 768]               0\n",
      "        LayerNorm-95             [-1, 197, 768]           1,536\n",
      "           Linear-96            [-1, 197, 3072]       2,362,368\n",
      "             GELU-97            [-1, 197, 3072]               0\n",
      "          Dropout-98            [-1, 197, 3072]               0\n",
      "           Linear-99             [-1, 197, 768]       2,360,064\n",
      "         Dropout-100             [-1, 197, 768]               0\n",
      "     ResidualAdd-101             [-1, 197, 768]               0\n",
      "       LayerNorm-102             [-1, 197, 768]           1,536\n",
      "          Linear-103            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-104          [-1, 8, 197, 197]               0\n",
      "          Linear-105             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-106             [-1, 197, 768]               0\n",
      "         Dropout-107             [-1, 197, 768]               0\n",
      "     ResidualAdd-108             [-1, 197, 768]               0\n",
      "       LayerNorm-109             [-1, 197, 768]           1,536\n",
      "          Linear-110            [-1, 197, 3072]       2,362,368\n",
      "            GELU-111            [-1, 197, 3072]               0\n",
      "         Dropout-112            [-1, 197, 3072]               0\n",
      "          Linear-113             [-1, 197, 768]       2,360,064\n",
      "         Dropout-114             [-1, 197, 768]               0\n",
      "     ResidualAdd-115             [-1, 197, 768]               0\n",
      "       LayerNorm-116             [-1, 197, 768]           1,536\n",
      "          Linear-117            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-118          [-1, 8, 197, 197]               0\n",
      "          Linear-119             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-120             [-1, 197, 768]               0\n",
      "         Dropout-121             [-1, 197, 768]               0\n",
      "     ResidualAdd-122             [-1, 197, 768]               0\n",
      "       LayerNorm-123             [-1, 197, 768]           1,536\n",
      "          Linear-124            [-1, 197, 3072]       2,362,368\n",
      "            GELU-125            [-1, 197, 3072]               0\n",
      "         Dropout-126            [-1, 197, 3072]               0\n",
      "          Linear-127             [-1, 197, 768]       2,360,064\n",
      "         Dropout-128             [-1, 197, 768]               0\n",
      "     ResidualAdd-129             [-1, 197, 768]               0\n",
      "       LayerNorm-130             [-1, 197, 768]           1,536\n",
      "          Linear-131            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-132          [-1, 8, 197, 197]               0\n",
      "          Linear-133             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-134             [-1, 197, 768]               0\n",
      "         Dropout-135             [-1, 197, 768]               0\n",
      "     ResidualAdd-136             [-1, 197, 768]               0\n",
      "       LayerNorm-137             [-1, 197, 768]           1,536\n",
      "          Linear-138            [-1, 197, 3072]       2,362,368\n",
      "            GELU-139            [-1, 197, 3072]               0\n",
      "         Dropout-140            [-1, 197, 3072]               0\n",
      "          Linear-141             [-1, 197, 768]       2,360,064\n",
      "         Dropout-142             [-1, 197, 768]               0\n",
      "     ResidualAdd-143             [-1, 197, 768]               0\n",
      "       LayerNorm-144             [-1, 197, 768]           1,536\n",
      "          Linear-145            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-146          [-1, 8, 197, 197]               0\n",
      "          Linear-147             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-148             [-1, 197, 768]               0\n",
      "         Dropout-149             [-1, 197, 768]               0\n",
      "     ResidualAdd-150             [-1, 197, 768]               0\n",
      "       LayerNorm-151             [-1, 197, 768]           1,536\n",
      "          Linear-152            [-1, 197, 3072]       2,362,368\n",
      "            GELU-153            [-1, 197, 3072]               0\n",
      "         Dropout-154            [-1, 197, 3072]               0\n",
      "          Linear-155             [-1, 197, 768]       2,360,064\n",
      "         Dropout-156             [-1, 197, 768]               0\n",
      "     ResidualAdd-157             [-1, 197, 768]               0\n",
      "       LayerNorm-158             [-1, 197, 768]           1,536\n",
      "          Linear-159            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-160          [-1, 8, 197, 197]               0\n",
      "          Linear-161             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-162             [-1, 197, 768]               0\n",
      "         Dropout-163             [-1, 197, 768]               0\n",
      "     ResidualAdd-164             [-1, 197, 768]               0\n",
      "       LayerNorm-165             [-1, 197, 768]           1,536\n",
      "          Linear-166            [-1, 197, 3072]       2,362,368\n",
      "            GELU-167            [-1, 197, 3072]               0\n",
      "         Dropout-168            [-1, 197, 3072]               0\n",
      "          Linear-169             [-1, 197, 768]       2,360,064\n",
      "         Dropout-170             [-1, 197, 768]               0\n",
      "     ResidualAdd-171             [-1, 197, 768]               0\n",
      "          Reduce-172                  [-1, 768]               0\n",
      "       LayerNorm-173                  [-1, 768]           1,536\n",
      "          Linear-174                 [-1, 1000]         769,000\n",
      "================================================================\n",
      "Total params: 86,415,592\n",
      "Trainable params: 86,415,592\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 364.33\n",
      "Params size (MB): 329.65\n",
      "Estimated Total Size (MB): 694.56\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class TransformerEncoder(nn.Sequential):\n",
    "  def __init__(self, depth: int = 12, **kwargs):\n",
    "    super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])\n",
    "\n",
    "class ClassificationHead(nn.Sequential):\n",
    "  def __init__(self, emb_size: int = 768, n_classes: int = 1000):\n",
    "    super().__init__(\n",
    "      Reduce('b n e -> b e', reduction='mean'), # [8, 196, 768] -> [8, 768]\n",
    "      nn.LayerNorm(emb_size),\n",
    "      nn.Linear(emb_size, n_classes)\n",
    "    )\n",
    "\n",
    "class ViT(nn.Sequential)    :\n",
    "  def __init__(self, in_channels: int=3, patch_size: int=16, emb_size:int = 768, \n",
    "               img_size: int = 224, depth: int = 12, n_classes: int = 1000,\n",
    "               **kwargs):\n",
    "      \n",
    "      super().__init__(\n",
    "        PatchEmbedding(in_channels, patch_size, emb_size, img_size),\n",
    "        TransformerEncoder(depth, emb_size=emb_size, **kwargs),\n",
    "        ClassificationHead(emb_size, n_classes)\n",
    "      )\n",
    "\n",
    "summary(ViT(), (3, 224, 224), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
